{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    POINT_SPLITTER = 9\n",
    "    ACE = 1\n",
    "    THREE = 3\n",
    "    POINTS = 8\n",
    "    POINTS_VALUE = 6\n",
    "\n",
    "    def __init__(self, number) -> None:\n",
    "        self.number = number\n",
    "    \n",
    "    def getSeed(self) -> int:\n",
    "        return self.number // 10\n",
    "    \n",
    "    def getValue(self) -> int:\n",
    "        return (self.number % 10) + 1\n",
    "    \n",
    "    def getPoints(self) -> int:\n",
    "        match self.getValue():\n",
    "            case self.ACE: return 11\n",
    "            case self.THREE: return 10\n",
    "            case x if x < self.POINTS: return 0\n",
    "            case x: return x - self.POINTS_VALUE\n",
    "    \n",
    "    def getState(self) -> tuple:\n",
    "        match self.getPoints():\n",
    "                case 0 : cardZone = 0 \n",
    "                case x if x < self.POINT_SPLITTER: cardZone = 1\n",
    "                case _: cardZone = 2\n",
    "             \n",
    "        return (self.getSeed(), cardZone)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deck:\n",
    "    DECK_CARDS = 40\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.cards = []\n",
    "        for i in range(0,self.DECK_CARDS):\n",
    "            self.cards.append(Card(i))\n",
    "        \n",
    "        # randomizing the deck\n",
    "        rnd.shuffle(self.cards)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.__init__()\n",
    "    \n",
    "    def draw(self) -> Card:\n",
    "        return self.cards.pop(0)\n",
    "\n",
    "    def cardsLeft(self) -> int:\n",
    "        return len(self.cards)\n",
    "    \n",
    "    def getLastCard(self) -> Card:\n",
    "        return self.cards[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    HAND_MAX_CARD = 3\n",
    "    CARD_NULL_VALUE = (4,3)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.hand = [] # 3 x Cards (seed = 4 and value = 3 mean null value)\n",
    "        self.oppoOverThreshold = 0 #[0, 1]\n",
    "        self.points = 0\n",
    "        self.wins = 0\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.hand = [] \n",
    "        self.oppoOverThreshold = 0\n",
    "        self.points = 0\n",
    "    \n",
    "    def getState(self) -> tuple:\n",
    "        handState = ()\n",
    "        for card in self.hand:\n",
    "            handState = handState + card.getState()\n",
    "\n",
    "        # enter null values for each missing card\n",
    "        missingCards = self.HAND_MAX_CARD - len(self.hand) \n",
    "        for _ in range(missingCards):\n",
    "            handState = handState + self.CARD_NULL_VALUE\n",
    "\n",
    "        return (self.oppoOverThreshold,) + handState\n",
    "    \n",
    "    def getCard(self, index) -> Card:\n",
    "        if index > len(self.hand) - 1: print(index, len(self.hand))\n",
    "        return self.hand[index]\n",
    "    \n",
    "    def addCard(self, card) -> None:\n",
    "        self.hand.append(card)\n",
    "    \n",
    "    def removeCard(self, index) -> Card:\n",
    "        return self.hand.pop(index)\n",
    "\n",
    "    def toggleOppoOverThreshold(self) -> None:\n",
    "        self.oppoOverThreshold = 1\n",
    "    \n",
    "    def victoryPassed(self) -> bool:\n",
    "        return self.points > 60\n",
    "    \n",
    "    def getPoints(self) -> int:\n",
    "        return self.points\n",
    "    \n",
    "    def addPoints(self, points) -> None:\n",
    "        self.points += points\n",
    "    \n",
    "    def remainingCards(self) -> int:\n",
    "        return len(self.hand)\n",
    "    \n",
    "    def addWin(self) -> None:\n",
    "        self.wins += 1\n",
    "    \n",
    "    def resetWins(self) -> int:\n",
    "        tmp = self.wins\n",
    "        self.wins = 0\n",
    "        return tmp\n",
    "    \n",
    "    def handIsEmpty(self) -> bool:\n",
    "        return len(self.hand) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    WIN_REWARD = 1\n",
    "    VICTORY_THRESHOLD = 45\n",
    "    BRISCOLA_THRESHOLD = 10\n",
    "    BRISCOLE_THRESHOLD = 7\n",
    "    \n",
    "    Q_STATUS_DIM = (2, 4, 2) + (2, 2, 2, 2) + (2, 5, 4, 5, 4, 5, 4) + (5, 4)\n",
    "    ACTION_DIM = (3,)\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.briscolaOverThreshold = 0  # [0, 1]\n",
    "        self.briscolaSeed = None  # [0, 1, 2, 3]\n",
    "        self.briscoleOut = 0\n",
    "        self.briscoleOverThreshold = 0 # [0, 1]\n",
    "        self.loadBySeed = [0, 0, 0, 0] # (denara, spade, bastoni, coppe) [0, 1]\n",
    "        \n",
    "        self.deck = Deck()\n",
    "        self.players = (Player(), Player())\n",
    "    \n",
    "    def getShape(self) -> tuple:\n",
    "        return self.Q_STATUS_DIM + self.ACTION_DIM\n",
    "            \n",
    "    def envState(self) -> tuple:\n",
    "        return (self.briscolaOverThreshold, self.briscolaSeed, self.briscoleOverThreshold) + tuple(self.loadBySeed)\n",
    "    \n",
    "    def getState(self, index) -> tuple:\n",
    "        return self.envState() + self.players[index].getState()\n",
    "    \n",
    "    def getActionState(self, action, index):\n",
    "        return self.players[index].getCard(action).getState()\n",
    "    \n",
    "    def reset(self) -> tuple:\n",
    "        for player in self.players:\n",
    "            player.reset()\n",
    "        self.deck.reset()\n",
    "\n",
    "        # resetting game state info\n",
    "        self.loadBySeed = [0, 0, 0, 0]\n",
    "        self.briscoleOverThreshold = 0\n",
    "\n",
    "        # updating new briscola info\n",
    "        self.briscolaSeed = self.deck.getLastCard().getSeed()\n",
    "        self.briscolaOverThreshold = int(self.deck.getLastCard().getPoints() >= self.BRISCOLA_THRESHOLD)\n",
    "\n",
    "        # dealing cards to the players\n",
    "        for _ in range(3):\n",
    "            for player in self.players:\n",
    "                player.addCard(self.deck.draw())\n",
    "\n",
    "        # returns the pair (fstPlayer state, sndPlayer state)\n",
    "        return (self.getState(0), self.getState(1)) \n",
    "    \n",
    "    def processPlays(self, fstPlay, sndPlay, fstPlayer, sndPlayer) -> tuple:\n",
    "        totPoints = (fstPlay.getPoints() + sndPlay.getPoints())\n",
    "       \n",
    "        winner = sndPlayer\n",
    "        if fstPlay.getSeed() == sndPlay.getSeed():\n",
    "            if fstPlay.getValue() > sndPlay.getValue(): winner = fstPlayer\n",
    "        elif sndPlay.getSeed() != self.briscolaSeed: winner = fstPlayer\n",
    "        \n",
    "        return (winner, totPoints)\n",
    "    \n",
    "    def stateUpdate(self, plays) -> None:\n",
    "        if self.players[0].getPoints() > self.VICTORY_THRESHOLD: self.players[1].toggleOppoOverThreshold()\n",
    "        if self.players[1].getPoints() > self.VICTORY_THRESHOLD: self.players[0].toggleOppoOverThreshold()\n",
    "\n",
    "        # updating briscola counter\n",
    "        for play in plays:\n",
    "            if play.getSeed() == self.briscolaSeed: \n",
    "                self.briscoleOut += 1\n",
    "            if play.getPoints() >= 10: self.loadBySeed[play.getSeed()] = 1\n",
    "        \n",
    "        if self.briscoleOut > self.briscoleOverThreshold: self.briscoleOverThreshold = 1\n",
    "\n",
    "    def step(self, fstPlayerAction, fstPlayerIndex, sndPlayerAction, sndPlayerIndex) -> tuple:\n",
    "        fstPlay = self.players[fstPlayerIndex].removeCard(fstPlayerAction)\n",
    "        sndPlay = self.players[sndPlayerIndex].removeCard(sndPlayerAction)\n",
    "\n",
    "        # evaluating plays and updating points\n",
    "        stepWinner, points = self.processPlays(fstPlay, sndPlay, fstPlayerIndex, sndPlayerIndex)\n",
    "        self.players[stepWinner].addPoints(points)\n",
    "        #print(stepWinner, fstPlayerIndex, (fstPlay.getValue(), fstPlay.getSeed()), (sndPlay.getValue(), sndPlay.getSeed()), self.briscolaSeed)\n",
    "\n",
    "        # generating rewards\n",
    "        #if stepWinner == fstPlayerIndex: rewards = [points, -points]\n",
    "        #else: rewards = [-points, points]\n",
    "        rewards = [0,0]\n",
    "        # updating state info\n",
    "        self.stateUpdate([fstPlay, sndPlay])\n",
    "\n",
    "        # dealing cards\n",
    "        if self.deck.cardsLeft() != 0:\n",
    "            self.players[0].addCard(self.deck.draw())\n",
    "            self.players[1].addCard(self.deck.draw())\n",
    "\n",
    "        # checking players victory\n",
    "        done = False\n",
    "        \n",
    "        for i in range(len(self.players)):\n",
    "            if self.players[i].victoryPassed():\n",
    "                done = True\n",
    "                self.players[i].addWin()\n",
    "                if i == fstPlayerIndex: \n",
    "                    rewards[0] += self.WIN_REWARD\n",
    "                    rewards[1] -= self.WIN_REWARD\n",
    "                else: \n",
    "                    rewards[0] -= self.WIN_REWARD\n",
    "                    rewards[1] += self.WIN_REWARD\n",
    "                break\n",
    "        \n",
    "        # nobody won if both players has empty hand\n",
    "        if self.players[0].handIsEmpty() and not done:\n",
    "            done = True\n",
    "            \n",
    "        return (self.getState(fstPlayerIndex), self.getState(sndPlayerIndex), rewards[0], rewards[1], \n",
    "                done, self.players[fstPlayerIndex].remainingCards(), self.players[sndPlayerIndex].remainingCards(), stepWinner)\n",
    "    \n",
    "    def getMatchStats(self) -> tuple:\n",
    "        return (self.players[0].resetWins(), self.players[1].resetWins())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IA:\n",
    "    EPS_THRESHOLD = 0.1\n",
    "    TEST_THRESHOLD = 100000\n",
    "    TEST_EPISODES = 10000\n",
    "    HAND_MAX_CARD = 3\n",
    "    NULL_OPPO_PLAY = (4,3)\n",
    "    FST_PLAYER_ID = 0\n",
    "    SND_PLAYER_ID = 1\n",
    "\n",
    "    def __init__(self, policyImport = False) -> None:\n",
    "        self.env = Environment()\n",
    "        # briscola need two players\n",
    "        # training player\n",
    "        QShape = self.env.getShape()\n",
    "        if policyImport: \n",
    "            self.Q = None\n",
    "            self.importPolicy()\n",
    "        else: self.Q = np.zeros(QShape) \n",
    "        \n",
    "        # trained player\n",
    "        self.trainedQ = self.Q.copy() \n",
    "\n",
    "    \n",
    "    def randomAction(self, remainingCards = HAND_MAX_CARD) -> int:\n",
    "        return rnd.randint(0, remainingCards-1)\n",
    "\n",
    "    def epsGreedy(self, state, Q, eps=0.1, remainingCards = HAND_MAX_CARD) -> int:\n",
    "        # Epsilon greedy policy\n",
    "        if np.random.uniform(0,1) < eps:\n",
    "            # Choose a random action\n",
    "            return self.randomAction(remainingCards)\n",
    "        else:\n",
    "            # Choose the action of a greedy policy\n",
    "            return self.greedy(state, Q, remainingCards)\n",
    "\n",
    "\n",
    "    def greedy(self, state, Q, remainingCards = HAND_MAX_CARD) -> int:\n",
    "        #Greedy policy\n",
    "        #return the index corresponding to the maximum action-state value\n",
    "        return np.argmax(Q[state][:remainingCards])\n",
    "    \n",
    "    def runTest(self, numEpisodes=100, toPrint=False) -> tuple:\n",
    "        # Run some episodes to test the policy against random player\n",
    "        # in this case Q2 represents random player\n",
    "        rewards = []\n",
    "        fstStarting = True\n",
    "\n",
    "        for _ in range(numEpisodes):\n",
    "            done = False\n",
    "            episodeRew = 0\n",
    "            \n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            # choosing who starts the game\n",
    "            if(fstStarting):\n",
    "                # generating fstPlayer action\n",
    "                state = state + self.NULL_OPPO_PLAY # null value on the opponent play\n",
    "                fstAction = self.greedy(state, self.Q)\n",
    "                    \n",
    "                # generating sndPlayer action\n",
    "                sndAction = self.randomAction()\n",
    "                \n",
    "                # take one step in the environment\n",
    "                state, _, stepReward, _, done, fstCards, sndCards, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "            else:\n",
    "                # generating sndPlayer action\n",
    "                sndAction = self.randomAction()\n",
    "\n",
    "                #  generating fstPlayer action\n",
    "                state = state + self.env.getActionState(sndAction, self.SND_PLAYER_ID)\n",
    "                fstAction = self.greedy(state, self.Q)\n",
    "                    \n",
    "                # take one step in the environment\n",
    "                _, state, _, stepReward, done, sndCards, fstCards, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "            \n",
    "            episodeRew += stepReward\n",
    "\n",
    "            #playing the game\n",
    "            while not done:\n",
    "                # if fstPlayer is playing first\n",
    "                if(stepWinner == self.FST_PLAYER_ID):\n",
    "                    # generating fstPLayer action\n",
    "                    state = state + self.NULL_OPPO_PLAY # null value on the opponent play\n",
    "                    fstAction = self.greedy(state, self.Q, fstCards)\n",
    "\n",
    "                    # generating sndPLayer action\n",
    "                    sndAction = self.randomAction(sndCards)\n",
    "                    \n",
    "                    # take one step in the environment\n",
    "                    state, _, stepReward, _, done, fstCards, sndCards, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "                \n",
    "                else: # if fstPlayer is playing second\n",
    "                    # generating sndPLayer action\n",
    "                    sndAction = self.randomAction(sndCards)\n",
    "\n",
    "                    #  generating fstPLayer action\n",
    "                    state = state + self.env.getActionState(sndAction, self.SND_PLAYER_ID)\n",
    "                    fstAction = self.greedy(state, self.Q, fstCards)\n",
    "\n",
    "                    # take one step in the environment\n",
    "                    _, state, _, stepReward, done, sndCards, fstCards, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "\n",
    "                episodeRew += stepReward\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(episodeRew)\n",
    "                    fstStarting = not fstStarting\n",
    "\n",
    "        matchStats = self.env.getMatchStats()\n",
    "        winPercentage =  (matchStats[0] * 100) / numEpisodes\n",
    "\n",
    "        if toPrint:\n",
    "            print('Mean score: %.3f Win percentage: %.2f out of %i games!'%(np.mean(rewards), winPercentage, numEpisodes))\n",
    "\n",
    "        return (np.mean(rewards), winPercentage)\n",
    "    \n",
    "    # research for optimal policy Q\n",
    "    def sarsaLearning(self, learningTime=60*10, alpha=0.1, eps=0.3, gamma=0.95, epsDecay=0.05) -> None:\n",
    "        \n",
    "        fstStarting = True\n",
    "        ep = 0\n",
    "\n",
    "        #for ep in range(numEpisodes):\n",
    "        start = time.time()\n",
    "        while time.time() - start < learningTime:\n",
    "            fstState, sndState = self.env.reset() # initial iniziale for Q1 and Q2\n",
    "            done = False\n",
    "            ep += 1\n",
    "\n",
    "            # choosing who starts the game\n",
    "            if(fstStarting):\n",
    "                # generating fstPlayer action\n",
    "                fstState = fstState + self.NULL_OPPO_PLAY # null value on the opponent play\n",
    "                fstAction = self.epsGreedy(fstState, self.Q, eps)\n",
    "                    \n",
    "                # generating sndPlayer action\n",
    "                sndState = sndState + self.env.getActionState(fstAction, self.FST_PLAYER_ID)\n",
    "                sndAction = self.greedy(sndState, self.trainedQ)\n",
    "                \n",
    "                # take one step in the environment\n",
    "                fstNextState, sndNextState, stepReward, _, done, fstCards, sndCards, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "            else:\n",
    "                # generating sndPlayer action\n",
    "                sndState = sndState + self.NULL_OPPO_PLAY\n",
    "                sndAction = self.greedy(sndState, self.trainedQ)\n",
    "\n",
    "                #  generating fstPLayer action\n",
    "                fstState = fstState + self.env.getActionState(sndAction, self.SND_PLAYER_ID)\n",
    "                fstAction = self.epsGreedy(fstState, self.Q, eps)\n",
    "                    \n",
    "                # take one step in the environment\n",
    "                sndNextState, fstNextState, _, stepReward, done, sndCards, fstCards, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "\n",
    "            # loop the main body until the environment stops\n",
    "            while not done:\n",
    "                # checking who's next playing first  \n",
    "                # if first player won this round must play first            \n",
    "                if(stepWinner == self.FST_PLAYER_ID): \n",
    "                    # generating first player next action (needed for the SARSA update)\n",
    "                    fstNextState = fstNextState + self.NULL_OPPO_PLAY\n",
    "                    fstNextAction = self.epsGreedy(fstNextState, self.Q, eps, fstCards)\n",
    "                \n",
    "                    # generating second player next action\n",
    "                    sndNextState = sndNextState + self.env.getActionState(fstNextAction, self.FST_PLAYER_ID)\n",
    "                    sndNextAction = self.greedy(sndNextState, self.trainedQ, sndCards)\n",
    "                \n",
    "                else: # if first player lost must play second \n",
    "                    # generating second player next action\n",
    "                    sndNextState = sndNextState + self.NULL_OPPO_PLAY\n",
    "                    sndNextAction = self.greedy(sndNextState, self.trainedQ, sndCards)\n",
    "\n",
    "                    # generating first player next action\n",
    "                    fstNextState = fstNextState + self.env.getActionState(sndNextAction, self.SND_PLAYER_ID)\n",
    "                    fstNextAction = self.epsGreedy(fstNextState, self.Q, eps, fstCards)\n",
    "\n",
    "                # SARSA update\n",
    "                self.Q[fstState][fstAction] = (self.Q[fstState][fstAction] \n",
    "                                               + alpha * (stepReward + gamma * self.Q[fstNextState][fstNextAction] \n",
    "                                                          - self.Q[fstState][fstAction]))\n",
    "\n",
    "                # updating players states and actions\n",
    "                fstState, sndState = fstNextState, sndNextState\n",
    "                fstAction, sndAction = fstNextAction, sndNextAction\n",
    "\n",
    "                # take one step in the environment\n",
    "                if(stepWinner == self.FST_PLAYER_ID):       \n",
    "                    fstNextState, sndNextState, stepReward, _, done, fstCards, sndCards, stepWinner = self.env.step(fstAction, self.FST_PLAYER_ID, sndAction, self.SND_PLAYER_ID)\n",
    "                else:\n",
    "                    sndNextState, fstNextState, _, stepReward, done, sndCards, fstCards, stepWinner = self.env.step(sndAction, self.SND_PLAYER_ID, fstAction, self.FST_PLAYER_ID)\n",
    "            \n",
    "            # every few episodes trainedQ is updated\n",
    "            if (ep % self.TEST_THRESHOLD) == 0:\n",
    "                # updating trainedQ\n",
    "                self.trainedQ = self.Q.copy()\n",
    "\n",
    "                # decay the epsilon value until it reaches the threshold\n",
    "                if eps > self.EPS_THRESHOLD: eps -= epsDecay\n",
    "\n",
    "                # training stats\n",
    "                trainingStats = self.env.getMatchStats()\n",
    "\n",
    "                # testing policy Q against random player\n",
    "                avgRew, winPercentage = self.runTest(self.TEST_EPISODES)\n",
    "                print('Episode:%.5d  Epsylon: %.4f  Average Reward: %.4f Win Percentage: %.2f' %(ep, eps, avgRew, winPercentage))\n",
    "        self.savePolicy(ep, winPercentage)\n",
    "\n",
    "    def savePolicy(self, episodes, winPercentage):\n",
    "        with open(\"ia.pk1\", \"wb\") as fp:\n",
    "            pickle.dump(self.Q, fp)\n",
    "            fp.close()\n",
    "\n",
    "        with open(\"infos.pk1\", \"wb\") as fp:\n",
    "            infos = {\"Episodes\": episodes,\n",
    "                     \"Win Percentage\": winPercentage}\n",
    "            pickle.dump(infos, fp)\n",
    "            fp.close()\n",
    "        print(\"Done savings!\")\n",
    "        dir = os.getcwd()\n",
    "        AIDim = int((os.stat(dir+\"/ia.pk1\").st_size)/(1024*1024))\n",
    "        print(\"AI Dimension:\", AIDim, \"MB\")\n",
    "\n",
    "    def importPolicy(self):\n",
    "        with open('ia.pk1', 'rb') as fp:\n",
    "            self.Q = pickle.load(fp)\n",
    "            fp.close()\n",
    "        print(\"Done importing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing\n",
      "Episode:100000  Epsylon: 0.2900  Average Reward: 0.3964 Win Percentage: 68.99\n",
      "Episode:200000  Epsylon: 0.2800  Average Reward: 0.4289 Win Percentage: 70.63\n",
      "Episode:300000  Epsylon: 0.2700  Average Reward: 0.4442 Win Percentage: 71.45\n",
      "Episode:400000  Epsylon: 0.2600  Average Reward: 0.4795 Win Percentage: 73.20\n",
      "Episode:500000  Epsylon: 0.2500  Average Reward: 0.4524 Win Percentage: 71.95\n",
      "Episode:600000  Epsylon: 0.2400  Average Reward: 0.4904 Win Percentage: 73.77\n",
      "Episode:700000  Epsylon: 0.2300  Average Reward: 0.4694 Win Percentage: 72.85\n",
      "Episode:800000  Epsylon: 0.2200  Average Reward: 0.5060 Win Percentage: 74.54\n",
      "Episode:900000  Epsylon: 0.2100  Average Reward: 0.4942 Win Percentage: 73.96\n",
      "Episode:1000000  Epsylon: 0.2000  Average Reward: 0.5009 Win Percentage: 74.36\n",
      "Episode:1100000  Epsylon: 0.1900  Average Reward: 0.5016 Win Percentage: 74.34\n",
      "Episode:1200000  Epsylon: 0.1800  Average Reward: 0.4974 Win Percentage: 74.11\n",
      "Episode:1300000  Epsylon: 0.1700  Average Reward: 0.4982 Win Percentage: 74.13\n",
      "Episode:1400000  Epsylon: 0.1600  Average Reward: 0.4969 Win Percentage: 74.18\n",
      "Episode:1500000  Epsylon: 0.1500  Average Reward: 0.4987 Win Percentage: 74.11\n",
      "Episode:1600000  Epsylon: 0.1400  Average Reward: 0.5143 Win Percentage: 75.09\n",
      "Episode:1700000  Epsylon: 0.1300  Average Reward: 0.5025 Win Percentage: 74.41\n",
      "Episode:1800000  Epsylon: 0.1200  Average Reward: 0.5117 Win Percentage: 74.80\n",
      "Episode:1900000  Epsylon: 0.1100  Average Reward: 0.5342 Win Percentage: 76.06\n",
      "Episode:2000000  Epsylon: 0.1000  Average Reward: 0.5051 Win Percentage: 74.62\n",
      "Episode:2100000  Epsylon: 0.0900  Average Reward: 0.5099 Win Percentage: 74.77\n",
      "Episode:2200000  Epsylon: 0.0800  Average Reward: 0.5186 Win Percentage: 75.21\n",
      "Episode:2300000  Epsylon: 0.0700  Average Reward: 0.5107 Win Percentage: 74.80\n",
      "Episode:2400000  Epsylon: 0.0600  Average Reward: 0.5239 Win Percentage: 75.50\n",
      "Episode:2500000  Epsylon: 0.0500  Average Reward: 0.5011 Win Percentage: 74.24\n",
      "Episode:2600000  Epsylon: 0.0400  Average Reward: 0.5111 Win Percentage: 74.83\n",
      "Episode:2700000  Epsylon: 0.0300  Average Reward: 0.5085 Win Percentage: 74.72\n",
      "Episode:2800000  Epsylon: 0.0200  Average Reward: 0.5115 Win Percentage: 74.91\n",
      "Episode:2900000  Epsylon: 0.0100  Average Reward: 0.5091 Win Percentage: 74.66\n",
      "Episode:3000000  Epsylon: 0.0100  Average Reward: 0.5216 Win Percentage: 75.29\n",
      "Episode:3100000  Epsylon: 0.0100  Average Reward: 0.5271 Win Percentage: 75.68\n",
      "Episode:3200000  Epsylon: 0.0100  Average Reward: 0.5075 Win Percentage: 74.70\n",
      "Episode:3300000  Epsylon: 0.0100  Average Reward: 0.5194 Win Percentage: 75.30\n",
      "Episode:3400000  Epsylon: 0.0100  Average Reward: 0.5164 Win Percentage: 75.08\n",
      "Episode:3500000  Epsylon: 0.0100  Average Reward: 0.5117 Win Percentage: 74.87\n",
      "Episode:3600000  Epsylon: 0.0100  Average Reward: 0.5205 Win Percentage: 75.39\n",
      "Episode:3700000  Epsylon: 0.0100  Average Reward: 0.5073 Win Percentage: 74.65\n",
      "Episode:3800000  Epsylon: 0.0100  Average Reward: 0.5289 Win Percentage: 75.73\n",
      "Episode:3900000  Epsylon: 0.0100  Average Reward: 0.5313 Win Percentage: 75.98\n",
      "Episode:4000000  Epsylon: 0.0100  Average Reward: 0.5122 Win Percentage: 74.97\n",
      "Episode:4100000  Epsylon: 0.0100  Average Reward: 0.4898 Win Percentage: 73.75\n",
      "Episode:4200000  Epsylon: 0.0100  Average Reward: 0.5114 Win Percentage: 74.74\n",
      "Episode:4300000  Epsylon: 0.0100  Average Reward: 0.5094 Win Percentage: 74.65\n",
      "Episode:4400000  Epsylon: 0.0100  Average Reward: 0.5034 Win Percentage: 74.41\n",
      "Episode:4500000  Epsylon: 0.0100  Average Reward: 0.5227 Win Percentage: 75.43\n",
      "Episode:4600000  Epsylon: 0.0100  Average Reward: 0.5109 Win Percentage: 74.82\n",
      "Episode:4700000  Epsylon: 0.0100  Average Reward: 0.5058 Win Percentage: 74.61\n",
      "Episode:4800000  Epsylon: 0.0100  Average Reward: 0.5044 Win Percentage: 74.52\n",
      "Episode:4900000  Epsylon: 0.0100  Average Reward: 0.5211 Win Percentage: 75.35\n",
      "Episode:5000000  Epsylon: 0.0100  Average Reward: 0.5017 Win Percentage: 74.35\n",
      "Episode:5100000  Epsylon: 0.0100  Average Reward: 0.5072 Win Percentage: 74.63\n",
      "Episode:5200000  Epsylon: 0.0100  Average Reward: 0.5100 Win Percentage: 74.86\n",
      "Episode:5300000  Epsylon: 0.0100  Average Reward: 0.5212 Win Percentage: 75.37\n",
      "Episode:5400000  Epsylon: 0.0100  Average Reward: 0.5014 Win Percentage: 74.34\n",
      "Episode:5500000  Epsylon: 0.0100  Average Reward: 0.5025 Win Percentage: 74.41\n",
      "Episode:5600000  Epsylon: 0.0100  Average Reward: 0.5126 Win Percentage: 74.93\n",
      "Episode:5700000  Epsylon: 0.0100  Average Reward: 0.5181 Win Percentage: 75.30\n",
      "Episode:5800000  Epsylon: 0.0100  Average Reward: 0.5116 Win Percentage: 74.83\n",
      "Episode:5900000  Epsylon: 0.0100  Average Reward: 0.5101 Win Percentage: 74.75\n",
      "Episode:6000000  Epsylon: 0.0100  Average Reward: 0.4957 Win Percentage: 73.98\n",
      "Episode:6100000  Epsylon: 0.0100  Average Reward: 0.5082 Win Percentage: 74.72\n",
      "Episode:6200000  Epsylon: 0.0100  Average Reward: 0.5031 Win Percentage: 74.51\n",
      "Episode:6300000  Epsylon: 0.0100  Average Reward: 0.5118 Win Percentage: 74.89\n",
      "Episode:6400000  Epsylon: 0.0100  Average Reward: 0.4973 Win Percentage: 74.20\n",
      "Episode:6500000  Epsylon: 0.0100  Average Reward: 0.5162 Win Percentage: 75.12\n",
      "Episode:6600000  Epsylon: 0.0100  Average Reward: 0.4949 Win Percentage: 74.01\n",
      "Episode:6700000  Epsylon: 0.0100  Average Reward: 0.4904 Win Percentage: 73.77\n",
      "Episode:6800000  Epsylon: 0.0100  Average Reward: 0.4970 Win Percentage: 74.13\n",
      "Episode:6900000  Epsylon: 0.0100  Average Reward: 0.4967 Win Percentage: 74.06\n",
      "Episode:7000000  Epsylon: 0.0100  Average Reward: 0.4898 Win Percentage: 73.89\n",
      "Episode:7100000  Epsylon: 0.0100  Average Reward: 0.4841 Win Percentage: 73.44\n",
      "Episode:7200000  Epsylon: 0.0100  Average Reward: 0.5042 Win Percentage: 74.53\n",
      "Episode:7300000  Epsylon: 0.0100  Average Reward: 0.5053 Win Percentage: 74.47\n",
      "Episode:7400000  Epsylon: 0.0100  Average Reward: 0.4979 Win Percentage: 74.18\n",
      "Episode:7500000  Epsylon: 0.0100  Average Reward: 0.5011 Win Percentage: 74.42\n",
      "Episode:7600000  Epsylon: 0.0100  Average Reward: 0.4997 Win Percentage: 74.20\n",
      "Episode:7700000  Epsylon: 0.0100  Average Reward: 0.4958 Win Percentage: 74.07\n",
      "Episode:7800000  Epsylon: 0.0100  Average Reward: 0.5066 Win Percentage: 74.63\n",
      "Episode:7900000  Epsylon: 0.0100  Average Reward: 0.4934 Win Percentage: 73.96\n",
      "Episode:8000000  Epsylon: 0.0100  Average Reward: 0.4926 Win Percentage: 74.00\n",
      "Episode:8100000  Epsylon: 0.0100  Average Reward: 0.4814 Win Percentage: 73.30\n",
      "Episode:8200000  Epsylon: 0.0100  Average Reward: 0.5028 Win Percentage: 74.46\n",
      "Episode:8300000  Epsylon: 0.0100  Average Reward: 0.4928 Win Percentage: 73.89\n",
      "Episode:8400000  Epsylon: 0.0100  Average Reward: 0.4815 Win Percentage: 73.45\n",
      "Episode:8500000  Epsylon: 0.0100  Average Reward: 0.4884 Win Percentage: 73.77\n",
      "Episode:8600000  Epsylon: 0.0100  Average Reward: 0.4772 Win Percentage: 73.17\n",
      "Episode:8700000  Epsylon: 0.0100  Average Reward: 0.4834 Win Percentage: 73.43\n",
      "Episode:8800000  Epsylon: 0.0100  Average Reward: 0.4779 Win Percentage: 73.26\n",
      "Episode:8900000  Epsylon: 0.0100  Average Reward: 0.4713 Win Percentage: 72.98\n",
      "Episode:9000000  Epsylon: 0.0100  Average Reward: 0.4920 Win Percentage: 73.92\n",
      "Episode:9100000  Epsylon: 0.0100  Average Reward: 0.4922 Win Percentage: 74.01\n",
      "Episode:9200000  Epsylon: 0.0100  Average Reward: 0.4936 Win Percentage: 73.93\n",
      "Episode:9300000  Epsylon: 0.0100  Average Reward: 0.4806 Win Percentage: 73.32\n",
      "Episode:9400000  Epsylon: 0.0100  Average Reward: 0.4895 Win Percentage: 73.76\n",
      "Episode:9500000  Epsylon: 0.0100  Average Reward: 0.4960 Win Percentage: 74.17\n",
      "Episode:9600000  Epsylon: 0.0100  Average Reward: 0.4854 Win Percentage: 73.64\n",
      "Episode:9700000  Epsylon: 0.0100  Average Reward: 0.4899 Win Percentage: 73.80\n",
      "Episode:9800000  Epsylon: 0.0100  Average Reward: 0.4818 Win Percentage: 73.35\n",
      "Episode:9900000  Epsylon: 0.0100  Average Reward: 0.4995 Win Percentage: 74.31\n",
      "Episode:10000000  Epsylon: 0.0100  Average Reward: 0.4902 Win Percentage: 73.74\n",
      "Episode:10100000  Epsylon: 0.0100  Average Reward: 0.4811 Win Percentage: 73.33\n",
      "Episode:10200000  Epsylon: 0.0100  Average Reward: 0.4738 Win Percentage: 72.95\n",
      "Episode:10300000  Epsylon: 0.0100  Average Reward: 0.4847 Win Percentage: 73.48\n",
      "Episode:10400000  Epsylon: 0.0100  Average Reward: 0.4759 Win Percentage: 73.16\n",
      "Episode:10500000  Epsylon: 0.0100  Average Reward: 0.4836 Win Percentage: 73.54\n",
      "Episode:10600000  Epsylon: 0.0100  Average Reward: 0.4933 Win Percentage: 74.00\n",
      "Episode:10700000  Epsylon: 0.0100  Average Reward: 0.4833 Win Percentage: 73.43\n",
      "Episode:10800000  Epsylon: 0.0100  Average Reward: 0.4820 Win Percentage: 73.39\n",
      "Episode:10900000  Epsylon: 0.0100  Average Reward: 0.4890 Win Percentage: 73.86\n",
      "Episode:11000000  Epsylon: 0.0100  Average Reward: 0.4734 Win Percentage: 72.95\n",
      "Episode:11100000  Epsylon: 0.0100  Average Reward: 0.4893 Win Percentage: 73.80\n",
      "Episode:11200000  Epsylon: 0.0100  Average Reward: 0.4845 Win Percentage: 73.35\n",
      "Episode:11300000  Epsylon: 0.0100  Average Reward: 0.4827 Win Percentage: 73.46\n",
      "Episode:11400000  Epsylon: 0.0100  Average Reward: 0.4763 Win Percentage: 73.17\n",
      "Episode:11500000  Epsylon: 0.0100  Average Reward: 0.4737 Win Percentage: 72.86\n",
      "Episode:11600000  Epsylon: 0.0100  Average Reward: 0.4851 Win Percentage: 73.63\n",
      "Episode:11700000  Epsylon: 0.0100  Average Reward: 0.4779 Win Percentage: 73.17\n",
      "Episode:11800000  Epsylon: 0.0100  Average Reward: 0.4774 Win Percentage: 73.13\n",
      "Episode:11900000  Epsylon: 0.0100  Average Reward: 0.4710 Win Percentage: 72.90\n",
      "Episode:12000000  Epsylon: 0.0100  Average Reward: 0.4617 Win Percentage: 72.37\n",
      "Episode:12100000  Epsylon: 0.0100  Average Reward: 0.4779 Win Percentage: 73.18\n",
      "Episode:12200000  Epsylon: 0.0100  Average Reward: 0.4713 Win Percentage: 72.79\n",
      "Episode:12300000  Epsylon: 0.0100  Average Reward: 0.4847 Win Percentage: 73.44\n",
      "Episode:12400000  Epsylon: 0.0100  Average Reward: 0.4717 Win Percentage: 72.78\n",
      "Episode:12500000  Epsylon: 0.0100  Average Reward: 0.4667 Win Percentage: 72.52\n",
      "Episode:12600000  Epsylon: 0.0100  Average Reward: 0.4572 Win Percentage: 72.02\n",
      "Episode:12700000  Epsylon: 0.0100  Average Reward: 0.4913 Win Percentage: 73.87\n",
      "Episode:12800000  Epsylon: 0.0100  Average Reward: 0.4718 Win Percentage: 72.84\n",
      "Episode:12900000  Epsylon: 0.0100  Average Reward: 0.4698 Win Percentage: 72.76\n",
      "Episode:13000000  Epsylon: 0.0100  Average Reward: 0.4842 Win Percentage: 73.39\n",
      "Episode:13100000  Epsylon: 0.0100  Average Reward: 0.4728 Win Percentage: 72.83\n",
      "Episode:13200000  Epsylon: 0.0100  Average Reward: 0.4823 Win Percentage: 73.43\n",
      "Episode:13300000  Epsylon: 0.0100  Average Reward: 0.4882 Win Percentage: 73.57\n",
      "Episode:13400000  Epsylon: 0.0100  Average Reward: 0.4796 Win Percentage: 73.27\n",
      "Episode:13500000  Epsylon: 0.0100  Average Reward: 0.4559 Win Percentage: 72.08\n",
      "Episode:13600000  Epsylon: 0.0100  Average Reward: 0.4525 Win Percentage: 71.76\n",
      "Episode:13700000  Epsylon: 0.0100  Average Reward: 0.4640 Win Percentage: 72.44\n",
      "Episode:13800000  Epsylon: 0.0100  Average Reward: 0.4899 Win Percentage: 73.67\n",
      "Episode:13900000  Epsylon: 0.0100  Average Reward: 0.4742 Win Percentage: 73.04\n",
      "Episode:14000000  Epsylon: 0.0100  Average Reward: 0.4622 Win Percentage: 72.40\n",
      "Episode:14100000  Epsylon: 0.0100  Average Reward: 0.4469 Win Percentage: 71.62\n",
      "Episode:14200000  Epsylon: 0.0100  Average Reward: 0.4609 Win Percentage: 72.32\n",
      "Episode:14300000  Epsylon: 0.0100  Average Reward: 0.4818 Win Percentage: 73.34\n",
      "Episode:14400000  Epsylon: 0.0100  Average Reward: 0.4655 Win Percentage: 72.47\n",
      "Episode:14500000  Epsylon: 0.0100  Average Reward: 0.4730 Win Percentage: 72.92\n",
      "Episode:14600000  Epsylon: 0.0100  Average Reward: 0.4612 Win Percentage: 72.26\n",
      "Episode:14700000  Epsylon: 0.0100  Average Reward: 0.4720 Win Percentage: 72.86\n",
      "Episode:14800000  Epsylon: 0.0100  Average Reward: 0.4643 Win Percentage: 72.41\n",
      "Episode:14900000  Epsylon: 0.0100  Average Reward: 0.4764 Win Percentage: 73.03\n",
      "Episode:15000000  Epsylon: 0.0100  Average Reward: 0.4607 Win Percentage: 72.22\n",
      "Episode:15100000  Epsylon: 0.0100  Average Reward: 0.4648 Win Percentage: 72.45\n",
      "Episode:15200000  Epsylon: 0.0100  Average Reward: 0.4597 Win Percentage: 72.27\n",
      "Episode:15300000  Epsylon: 0.0100  Average Reward: 0.4615 Win Percentage: 72.29\n",
      "Episode:15400000  Epsylon: 0.0100  Average Reward: 0.4619 Win Percentage: 72.32\n",
      "Episode:15500000  Epsylon: 0.0100  Average Reward: 0.4630 Win Percentage: 72.38\n",
      "Episode:15600000  Epsylon: 0.0100  Average Reward: 0.4590 Win Percentage: 72.36\n",
      "Episode:15700000  Epsylon: 0.0100  Average Reward: 0.4633 Win Percentage: 72.37\n",
      "Episode:15800000  Epsylon: 0.0100  Average Reward: 0.4568 Win Percentage: 72.13\n",
      "Episode:15900000  Epsylon: 0.0100  Average Reward: 0.4657 Win Percentage: 72.62\n",
      "Episode:16000000  Epsylon: 0.0100  Average Reward: 0.4672 Win Percentage: 72.67\n",
      "Episode:16100000  Epsylon: 0.0100  Average Reward: 0.4598 Win Percentage: 72.26\n",
      "Episode:16200000  Epsylon: 0.0100  Average Reward: 0.4470 Win Percentage: 71.62\n",
      "Episode:16300000  Epsylon: 0.0100  Average Reward: 0.4668 Win Percentage: 72.67\n",
      "Episode:16400000  Epsylon: 0.0100  Average Reward: 0.4750 Win Percentage: 73.07\n",
      "Episode:16500000  Epsylon: 0.0100  Average Reward: 0.4616 Win Percentage: 72.37\n",
      "Episode:16600000  Epsylon: 0.0100  Average Reward: 0.4468 Win Percentage: 71.72\n",
      "Episode:16700000  Epsylon: 0.0100  Average Reward: 0.4502 Win Percentage: 71.81\n",
      "Episode:16800000  Epsylon: 0.0100  Average Reward: 0.4497 Win Percentage: 71.87\n",
      "Episode:16900000  Epsylon: 0.0100  Average Reward: 0.4704 Win Percentage: 72.79\n",
      "Episode:17000000  Epsylon: 0.0100  Average Reward: 0.4488 Win Percentage: 71.68\n",
      "Episode:17100000  Epsylon: 0.0100  Average Reward: 0.4501 Win Percentage: 71.82\n",
      "Episode:17200000  Epsylon: 0.0100  Average Reward: 0.4605 Win Percentage: 72.22\n",
      "Episode:17300000  Epsylon: 0.0100  Average Reward: 0.4683 Win Percentage: 72.76\n",
      "Episode:17400000  Epsylon: 0.0100  Average Reward: 0.4510 Win Percentage: 71.88\n",
      "Episode:17500000  Epsylon: 0.0100  Average Reward: 0.4578 Win Percentage: 72.12\n",
      "Episode:17600000  Epsylon: 0.0100  Average Reward: 0.4585 Win Percentage: 72.19\n",
      "Episode:17700000  Epsylon: 0.0100  Average Reward: 0.4612 Win Percentage: 72.28\n",
      "Episode:17800000  Epsylon: 0.0100  Average Reward: 0.4456 Win Percentage: 71.46\n",
      "Episode:17900000  Epsylon: 0.0100  Average Reward: 0.4606 Win Percentage: 72.28\n",
      "Episode:18000000  Epsylon: 0.0100  Average Reward: 0.4636 Win Percentage: 72.46\n",
      "Episode:18100000  Epsylon: 0.0100  Average Reward: 0.4665 Win Percentage: 72.67\n",
      "Episode:18200000  Epsylon: 0.0100  Average Reward: 0.4566 Win Percentage: 72.12\n",
      "Episode:18300000  Epsylon: 0.0100  Average Reward: 0.4573 Win Percentage: 72.06\n",
      "Episode:18400000  Epsylon: 0.0100  Average Reward: 0.4620 Win Percentage: 72.44\n",
      "Episode:18500000  Epsylon: 0.0100  Average Reward: 0.4549 Win Percentage: 72.07\n",
      "Episode:18600000  Epsylon: 0.0100  Average Reward: 0.4568 Win Percentage: 72.03\n",
      "Episode:18700000  Epsylon: 0.0100  Average Reward: 0.4482 Win Percentage: 71.70\n",
      "Episode:18800000  Epsylon: 0.0100  Average Reward: 0.4494 Win Percentage: 71.68\n",
      "Episode:18900000  Epsylon: 0.0100  Average Reward: 0.4606 Win Percentage: 72.31\n",
      "Episode:19000000  Epsylon: 0.0100  Average Reward: 0.4543 Win Percentage: 71.91\n",
      "Done savings!\n",
      "AI Dimension: 1875 MB\n"
     ]
    }
   ],
   "source": [
    "ia = IA(True)\n",
    "ia.sarsaLearning(60*60*1/2, 0.1, 0.3, 0.95, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "briscola",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
