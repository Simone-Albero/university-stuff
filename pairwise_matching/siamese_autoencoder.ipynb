{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLIER = 100\n",
    "MARGIN = MULTIPLIER/5\n",
    "ALPHA = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from utilities import CustomDataset, trainLoop, testLoop, save_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseAutoEncoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(384, 192),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(192, 96),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(96, 48)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(48, 96),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(96, 192),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(192, 384),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(384, 768)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        latent1 = self.encoder(x1)\n",
    "        reconstructed1 = self.decoder(latent1)\n",
    "        latent2 = self.encoder(x2)\n",
    "        reconstructed2 = self.decoder(latent2)\n",
    "\n",
    "        reconstruct_distance1 = torch.norm(x1-reconstructed1, dim=1)\n",
    "        reconstruct_distance2 = torch.norm(x2-reconstructed2, dim=1)\n",
    "        latend_distance = torch.norm(latent1-latent2, dim=1)\n",
    "        return reconstruct_distance1, reconstruct_distance2, latend_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HibridLoss(nn.Module):\n",
    "    def __init__(self, alpha, margin):\n",
    "        super(HibridLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, outputs, label):\n",
    "        reconstruct_distance1, reconstruct_distance2, latend_distance = outputs\n",
    "        loss_reconstruction = self.alpha * ((reconstruct_distance1**2)+(reconstruct_distance2**2))\n",
    "\n",
    "        loss_1 = label * (latend_distance**2)\n",
    "        loss_2 = (1-label)*(torch.max(self.margin-latend_distance, torch.zeros_like(latend_distance))**2)\n",
    "\n",
    "        loss_hibrid = loss_reconstruction + loss_1 + loss_2\n",
    "        return torch.mean(loss_hibrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVES_FOLDER = \"saves/\"\n",
    "df = pd.read_csv(SAVES_FOLDER + \"dataset.csv\")\n",
    "\n",
    "with open(SAVES_FOLDER + 'id2embedding.pkl', 'rb') as f:\n",
    "    id2embedding = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols = [\"left_spec_id\", \"right_spec_id\"]\n",
    "target_col = \"label\"\n",
    "\n",
    "dataset = CustomDataset(df, features_cols, target_col, id2embedding, MULTIPLIER, True)\n",
    "\n",
    "train_size = int(0.75 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "autoencoder = SiameseAutoEncoder()\n",
    "\n",
    "criterion = HibridLoss(alpha=ALPHA, margin=MARGIN)\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.00008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 500.19938771565756\n",
      "Test Loss: 220.5566, Test Accuracy: 0.4738\n",
      "Precision: 0.4105, Recall: 0.9463, F1-score: 0.5726\n",
      "TP: 282, FP: 405, TN: 97, FN: 16\n",
      "Epoch 2, Loss: 205.0536148325602\n",
      "Test Loss: 195.3644, Test Accuracy: 0.5038\n",
      "Precision: 0.4267, Recall: 0.9664, F1-score: 0.5920\n",
      "TP: 288, FP: 387, TN: 115, FN: 10\n",
      "Epoch 3, Loss: 177.2160090382894\n",
      "Test Loss: 176.3970, Test Accuracy: 0.5262\n",
      "Precision: 0.4370, Recall: 0.9430, F1-score: 0.5972\n",
      "TP: 281, FP: 362, TN: 140, FN: 17\n",
      "Epoch 4, Loss: 157.28212776184083\n",
      "Test Loss: 161.2208, Test Accuracy: 0.4988\n",
      "Precision: 0.4257, Recall: 0.9899, F1-score: 0.5954\n",
      "TP: 295, FP: 398, TN: 104, FN: 3\n",
      "Epoch 5, Loss: 148.0189002863566\n",
      "Test Loss: 157.4114, Test Accuracy: 0.5425\n",
      "Precision: 0.4474, Recall: 0.9698, F1-score: 0.6123\n",
      "TP: 289, FP: 357, TN: 145, FN: 9\n",
      "Epoch 6, Loss: 142.15649202982584\n",
      "Test Loss: 151.0299, Test Accuracy: 0.5312\n",
      "Precision: 0.4423, Recall: 0.9899, F1-score: 0.6114\n",
      "TP: 295, FP: 372, TN: 130, FN: 3\n",
      "Epoch 7, Loss: 136.7199174753825\n",
      "Test Loss: 148.1831, Test Accuracy: 0.5487\n",
      "Precision: 0.4516, Recall: 0.9866, F1-score: 0.6196\n",
      "TP: 294, FP: 357, TN: 145, FN: 4\n",
      "Epoch 8, Loss: 131.39889443715413\n",
      "Test Loss: 143.8449, Test Accuracy: 0.5737\n",
      "Precision: 0.4659, Recall: 0.9866, F1-score: 0.6329\n",
      "TP: 294, FP: 337, TN: 165, FN: 4\n",
      "Epoch 9, Loss: 126.19036838531494\n",
      "Test Loss: 137.5175, Test Accuracy: 0.5750\n",
      "Precision: 0.4669, Recall: 0.9933, F1-score: 0.6352\n",
      "TP: 296, FP: 338, TN: 164, FN: 2\n",
      "Epoch 10, Loss: 121.08718744913737\n",
      "Test Loss: 133.1041, Test Accuracy: 0.6038\n",
      "Precision: 0.4844, Recall: 0.9899, F1-score: 0.6505\n",
      "TP: 295, FP: 314, TN: 188, FN: 3\n",
      "Epoch 11, Loss: 115.74538487752278\n",
      "Test Loss: 130.3673, Test Accuracy: 0.6350\n",
      "Precision: 0.5052, Recall: 0.9832, F1-score: 0.6674\n",
      "TP: 293, FP: 287, TN: 215, FN: 5\n",
      "Epoch 12, Loss: 111.53468280792237\n",
      "Test Loss: 126.2963, Test Accuracy: 0.6412\n",
      "Precision: 0.5095, Recall: 0.9899, F1-score: 0.6727\n",
      "TP: 295, FP: 284, TN: 218, FN: 3\n",
      "Epoch 13, Loss: 108.30169185638428\n",
      "Test Loss: 123.6864, Test Accuracy: 0.6663\n",
      "Precision: 0.5279, Recall: 0.9832, F1-score: 0.6870\n",
      "TP: 293, FP: 262, TN: 240, FN: 5\n",
      "Epoch 14, Loss: 104.43371200561523\n",
      "Test Loss: 120.4914, Test Accuracy: 0.6775\n",
      "Precision: 0.5366, Recall: 0.9832, F1-score: 0.6943\n",
      "TP: 293, FP: 253, TN: 249, FN: 5\n",
      "Epoch 15, Loss: 101.99571539560954\n",
      "Test Loss: 119.6639, Test Accuracy: 0.6925\n",
      "Precision: 0.5492, Recall: 0.9732, F1-score: 0.7022\n",
      "TP: 290, FP: 238, TN: 264, FN: 8\n",
      "Epoch 16, Loss: 99.12643597920736\n",
      "Test Loss: 116.0035, Test Accuracy: 0.7200\n",
      "Precision: 0.5725, Recall: 0.9799, F1-score: 0.7228\n",
      "TP: 292, FP: 218, TN: 284, FN: 6\n",
      "Epoch 17, Loss: 96.189214108785\n",
      "Test Loss: 112.4167, Test Accuracy: 0.7300\n",
      "Precision: 0.5820, Recall: 0.9765, F1-score: 0.7293\n",
      "TP: 291, FP: 209, TN: 293, FN: 7\n",
      "Epoch 18, Loss: 93.95665687561035\n",
      "Test Loss: 110.8294, Test Accuracy: 0.7338\n",
      "Precision: 0.5855, Recall: 0.9765, F1-score: 0.7321\n",
      "TP: 291, FP: 206, TN: 296, FN: 7\n",
      "Epoch 19, Loss: 91.66944618225098\n",
      "Test Loss: 110.1276, Test Accuracy: 0.7325\n",
      "Precision: 0.5843, Recall: 0.9765, F1-score: 0.7312\n",
      "TP: 291, FP: 207, TN: 295, FN: 7\n",
      "Epoch 20, Loss: 89.55278863271077\n",
      "Test Loss: 107.4457, Test Accuracy: 0.7550\n",
      "Precision: 0.6062, Recall: 0.9765, F1-score: 0.7481\n",
      "TP: 291, FP: 189, TN: 313, FN: 7\n",
      "Epoch 21, Loss: 87.53605280558268\n",
      "Test Loss: 104.0900, Test Accuracy: 0.7488\n",
      "Precision: 0.6000, Recall: 0.9765, F1-score: 0.7433\n",
      "TP: 291, FP: 194, TN: 308, FN: 7\n",
      "Epoch 22, Loss: 85.368846244812\n",
      "Test Loss: 103.8770, Test Accuracy: 0.7638\n",
      "Precision: 0.6152, Recall: 0.9765, F1-score: 0.7549\n",
      "TP: 291, FP: 182, TN: 320, FN: 7\n",
      "Epoch 23, Loss: 83.4840852355957\n",
      "Test Loss: 100.0151, Test Accuracy: 0.7638\n",
      "Precision: 0.6152, Recall: 0.9765, F1-score: 0.7549\n",
      "TP: 291, FP: 182, TN: 320, FN: 7\n",
      "Epoch 24, Loss: 81.79609828948975\n",
      "Test Loss: 96.6360, Test Accuracy: 0.7750\n",
      "Precision: 0.6255, Recall: 0.9866, F1-score: 0.7656\n",
      "TP: 294, FP: 176, TN: 326, FN: 4\n",
      "Epoch 25, Loss: 80.2475334294637\n",
      "Test Loss: 99.3222, Test Accuracy: 0.7875\n",
      "Precision: 0.6385, Recall: 0.9899, F1-score: 0.7763\n",
      "TP: 295, FP: 167, TN: 335, FN: 3\n",
      "Epoch 26, Loss: 78.52265858968099\n",
      "Test Loss: 96.4336, Test Accuracy: 0.7987\n",
      "Precision: 0.6532, Recall: 0.9799, F1-score: 0.7839\n",
      "TP: 292, FP: 155, TN: 347, FN: 6\n",
      "Epoch 27, Loss: 76.70565216064453\n",
      "Test Loss: 95.2625, Test Accuracy: 0.8000\n",
      "Precision: 0.6554, Recall: 0.9765, F1-score: 0.7844\n",
      "TP: 291, FP: 153, TN: 349, FN: 7\n",
      "Epoch 28, Loss: 75.07378471374511\n",
      "Test Loss: 91.9636, Test Accuracy: 0.8075\n",
      "Precision: 0.6636, Recall: 0.9799, F1-score: 0.7913\n",
      "TP: 292, FP: 148, TN: 354, FN: 6\n",
      "Epoch 29, Loss: 73.67235824584961\n",
      "Test Loss: 92.6892, Test Accuracy: 0.8125\n",
      "Precision: 0.6697, Recall: 0.9799, F1-score: 0.7956\n",
      "TP: 292, FP: 144, TN: 358, FN: 6\n",
      "Epoch 30, Loss: 72.02376265207927\n",
      "Test Loss: 91.7112, Test Accuracy: 0.8213\n",
      "Precision: 0.6815, Recall: 0.9765, F1-score: 0.8028\n",
      "TP: 291, FP: 136, TN: 366, FN: 7\n",
      "Epoch 31, Loss: 71.09819770812989\n",
      "Test Loss: 88.6707, Test Accuracy: 0.8137\n",
      "Precision: 0.6721, Recall: 0.9765, F1-score: 0.7962\n",
      "TP: 291, FP: 142, TN: 360, FN: 7\n",
      "Epoch 32, Loss: 69.85621448516845\n",
      "Test Loss: 87.9826, Test Accuracy: 0.8187\n",
      "Precision: 0.6743, Recall: 0.9933, F1-score: 0.8033\n",
      "TP: 296, FP: 143, TN: 359, FN: 2\n",
      "Epoch 33, Loss: 68.39074460347493\n",
      "Test Loss: 86.8179, Test Accuracy: 0.8250\n",
      "Precision: 0.6846, Recall: 0.9832, F1-score: 0.8072\n",
      "TP: 293, FP: 135, TN: 367, FN: 5\n",
      "Epoch 34, Loss: 67.45889175415039\n",
      "Test Loss: 84.0319, Test Accuracy: 0.8200\n",
      "Precision: 0.6774, Recall: 0.9866, F1-score: 0.8033\n",
      "TP: 294, FP: 140, TN: 362, FN: 4\n",
      "Epoch 35, Loss: 65.94356986999512\n",
      "Test Loss: 84.1978, Test Accuracy: 0.8213\n",
      "Precision: 0.6807, Recall: 0.9799, F1-score: 0.8033\n",
      "TP: 292, FP: 137, TN: 365, FN: 6\n",
      "Epoch 36, Loss: 65.47432762145996\n",
      "Test Loss: 83.0975, Test Accuracy: 0.8287\n",
      "Precision: 0.6894, Recall: 0.9832, F1-score: 0.8105\n",
      "TP: 293, FP: 132, TN: 370, FN: 5\n",
      "Epoch 37, Loss: 64.12780404408772\n",
      "Test Loss: 82.7870, Test Accuracy: 0.8313\n",
      "Precision: 0.6927, Recall: 0.9832, F1-score: 0.8128\n",
      "TP: 293, FP: 130, TN: 372, FN: 5\n",
      "Epoch 38, Loss: 63.77492520650228\n",
      "Test Loss: 81.1091, Test Accuracy: 0.8363\n",
      "Precision: 0.6983, Recall: 0.9866, F1-score: 0.8178\n",
      "TP: 294, FP: 127, TN: 375, FN: 4\n",
      "Epoch 39, Loss: 62.03171451568603\n",
      "Test Loss: 81.3763, Test Accuracy: 0.8400\n",
      "Precision: 0.7053, Recall: 0.9799, F1-score: 0.8202\n",
      "TP: 292, FP: 122, TN: 380, FN: 6\n",
      "Epoch 40, Loss: 61.372962964375816\n",
      "Test Loss: 79.1366, Test Accuracy: 0.8512\n",
      "Precision: 0.7199, Recall: 0.9832, F1-score: 0.8312\n",
      "TP: 293, FP: 114, TN: 388, FN: 5\n",
      "Epoch 41, Loss: 60.595630747477216\n",
      "Test Loss: 80.0753, Test Accuracy: 0.8475\n",
      "Precision: 0.7115, Recall: 0.9933, F1-score: 0.8291\n",
      "TP: 296, FP: 120, TN: 382, FN: 2\n",
      "Epoch 42, Loss: 59.673416697184244\n",
      "Test Loss: 76.6712, Test Accuracy: 0.8538\n",
      "Precision: 0.7246, Recall: 0.9799, F1-score: 0.8331\n",
      "TP: 292, FP: 111, TN: 391, FN: 6\n",
      "Epoch 43, Loss: 59.40615568796794\n",
      "Test Loss: 75.6538, Test Accuracy: 0.8500\n",
      "Precision: 0.7150, Recall: 0.9933, F1-score: 0.8315\n",
      "TP: 296, FP: 118, TN: 384, FN: 2\n",
      "Epoch 44, Loss: 58.72134124755859\n",
      "Test Loss: 76.1303, Test Accuracy: 0.8562\n",
      "Precision: 0.7237, Recall: 0.9933, F1-score: 0.8373\n",
      "TP: 296, FP: 113, TN: 389, FN: 2\n",
      "Epoch 45, Loss: 57.896329968770345\n",
      "Test Loss: 75.7668, Test Accuracy: 0.8525\n",
      "Precision: 0.7228, Recall: 0.9799, F1-score: 0.8319\n",
      "TP: 292, FP: 112, TN: 390, FN: 6\n",
      "Epoch 46, Loss: 57.70352745691935\n",
      "Test Loss: 75.8492, Test Accuracy: 0.8538\n",
      "Precision: 0.7202, Recall: 0.9933, F1-score: 0.8350\n",
      "TP: 296, FP: 115, TN: 387, FN: 2\n",
      "Epoch 47, Loss: 56.79964913686116\n",
      "Test Loss: 75.0157, Test Accuracy: 0.8625\n",
      "Precision: 0.7338, Recall: 0.9899, F1-score: 0.8429\n",
      "TP: 295, FP: 107, TN: 395, FN: 3\n",
      "Epoch 48, Loss: 56.15759479522705\n",
      "Test Loss: 75.2953, Test Accuracy: 0.8662\n",
      "Precision: 0.7430, Recall: 0.9799, F1-score: 0.8452\n",
      "TP: 292, FP: 101, TN: 401, FN: 6\n",
      "Epoch 49, Loss: 55.736701253255205\n",
      "Test Loss: 73.5802, Test Accuracy: 0.8688\n",
      "Precision: 0.7419, Recall: 0.9933, F1-score: 0.8494\n",
      "TP: 296, FP: 103, TN: 399, FN: 2\n",
      "Epoch 50, Loss: 54.82234899520874\n",
      "Test Loss: 74.4623, Test Accuracy: 0.8725\n",
      "Precision: 0.7487, Recall: 0.9899, F1-score: 0.8526\n",
      "TP: 295, FP: 99, TN: 403, FN: 3\n",
      "Epoch 51, Loss: 54.07406139373779\n",
      "Test Loss: 71.6498, Test Accuracy: 0.8712\n",
      "Precision: 0.7468, Recall: 0.9899, F1-score: 0.8514\n",
      "TP: 295, FP: 100, TN: 402, FN: 3\n",
      "Epoch 52, Loss: 53.69383687973023\n",
      "Test Loss: 70.8456, Test Accuracy: 0.8712\n",
      "Precision: 0.7456, Recall: 0.9933, F1-score: 0.8518\n",
      "TP: 296, FP: 101, TN: 401, FN: 2\n",
      "Epoch 53, Loss: 53.12117251078288\n",
      "Test Loss: 69.5833, Test Accuracy: 0.8750\n",
      "Precision: 0.7513, Recall: 0.9933, F1-score: 0.8555\n",
      "TP: 296, FP: 98, TN: 404, FN: 2\n",
      "Epoch 54, Loss: 52.63371781667074\n",
      "Test Loss: 70.0969, Test Accuracy: 0.8800\n",
      "Precision: 0.7617, Recall: 0.9866, F1-score: 0.8596\n",
      "TP: 294, FP: 92, TN: 410, FN: 4\n",
      "Epoch 55, Loss: 52.17641049702962\n",
      "Test Loss: 68.1010, Test Accuracy: 0.8762\n",
      "Precision: 0.7532, Recall: 0.9933, F1-score: 0.8567\n",
      "TP: 296, FP: 97, TN: 405, FN: 2\n",
      "Epoch 56, Loss: 51.53804500579834\n",
      "Test Loss: 68.2743, Test Accuracy: 0.8862\n",
      "Precision: 0.7688, Recall: 0.9933, F1-score: 0.8668\n",
      "TP: 296, FP: 89, TN: 413, FN: 2\n",
      "Epoch 57, Loss: 50.71013753255208\n",
      "Test Loss: 67.9094, Test Accuracy: 0.8825\n",
      "Precision: 0.7629, Recall: 0.9933, F1-score: 0.8630\n",
      "TP: 296, FP: 92, TN: 410, FN: 2\n",
      "Epoch 58, Loss: 50.674359734853105\n",
      "Test Loss: 66.7416, Test Accuracy: 0.8825\n",
      "Precision: 0.7629, Recall: 0.9933, F1-score: 0.8630\n",
      "TP: 296, FP: 92, TN: 410, FN: 2\n",
      "Epoch 59, Loss: 49.70088771184285\n",
      "Test Loss: 67.0801, Test Accuracy: 0.8862\n",
      "Precision: 0.7688, Recall: 0.9933, F1-score: 0.8668\n",
      "TP: 296, FP: 89, TN: 413, FN: 2\n",
      "Epoch 60, Loss: 49.763275458017986\n",
      "Test Loss: 67.8111, Test Accuracy: 0.8900\n",
      "Precision: 0.7749, Recall: 0.9933, F1-score: 0.8706\n",
      "TP: 296, FP: 86, TN: 416, FN: 2\n",
      "Epoch 61, Loss: 48.93201956431071\n",
      "Test Loss: 66.3748, Test Accuracy: 0.8875\n",
      "Precision: 0.7723, Recall: 0.9899, F1-score: 0.8676\n",
      "TP: 295, FP: 87, TN: 415, FN: 3\n",
      "Epoch 62, Loss: 48.62683795928955\n",
      "Test Loss: 65.3495, Test Accuracy: 0.8938\n",
      "Precision: 0.7810, Recall: 0.9933, F1-score: 0.8744\n",
      "TP: 296, FP: 83, TN: 419, FN: 2\n",
      "Epoch 63, Loss: 48.117780666351315\n",
      "Test Loss: 63.4489, Test Accuracy: 0.8900\n",
      "Precision: 0.7749, Recall: 0.9933, F1-score: 0.8706\n",
      "TP: 296, FP: 86, TN: 416, FN: 2\n",
      "Epoch 64, Loss: 47.49110522588094\n",
      "Test Loss: 65.0726, Test Accuracy: 0.8850\n",
      "Precision: 0.7668, Recall: 0.9933, F1-score: 0.8655\n",
      "TP: 296, FP: 90, TN: 412, FN: 2\n",
      "Epoch 65, Loss: 47.42778670628866\n",
      "Test Loss: 62.8135, Test Accuracy: 0.8938\n",
      "Precision: 0.7810, Recall: 0.9933, F1-score: 0.8744\n",
      "TP: 296, FP: 83, TN: 419, FN: 2\n",
      "Epoch 66, Loss: 46.76876896540324\n",
      "Test Loss: 62.0178, Test Accuracy: 0.8938\n",
      "Precision: 0.7810, Recall: 0.9933, F1-score: 0.8744\n",
      "TP: 296, FP: 83, TN: 419, FN: 2\n",
      "Epoch 67, Loss: 46.20348333358765\n",
      "Test Loss: 61.9905, Test Accuracy: 0.9012\n",
      "Precision: 0.7936, Recall: 0.9933, F1-score: 0.8823\n",
      "TP: 296, FP: 77, TN: 425, FN: 2\n",
      "Epoch 68, Loss: 45.64403581619263\n",
      "Test Loss: 60.8525, Test Accuracy: 0.8938\n",
      "Precision: 0.7810, Recall: 0.9933, F1-score: 0.8744\n",
      "TP: 296, FP: 83, TN: 419, FN: 2\n",
      "Epoch 69, Loss: 45.27127669652303\n",
      "Test Loss: 63.0014, Test Accuracy: 0.8950\n",
      "Precision: 0.7831, Recall: 0.9933, F1-score: 0.8757\n",
      "TP: 296, FP: 82, TN: 420, FN: 2\n",
      "Epoch 70, Loss: 45.27997267405192\n",
      "Test Loss: 62.4768, Test Accuracy: 0.9050\n",
      "Precision: 0.8000, Recall: 0.9933, F1-score: 0.8862\n",
      "TP: 296, FP: 74, TN: 428, FN: 2\n",
      "Epoch 71, Loss: 44.429942207336424\n",
      "Test Loss: 59.7557, Test Accuracy: 0.8975\n",
      "Precision: 0.7872, Recall: 0.9933, F1-score: 0.8783\n",
      "TP: 296, FP: 80, TN: 422, FN: 2\n",
      "Epoch 72, Loss: 44.176452566782636\n",
      "Test Loss: 59.6086, Test Accuracy: 0.9000\n",
      "Precision: 0.7914, Recall: 0.9933, F1-score: 0.8810\n",
      "TP: 296, FP: 78, TN: 424, FN: 2\n",
      "Epoch 73, Loss: 43.598616739908856\n",
      "Test Loss: 59.6366, Test Accuracy: 0.9012\n",
      "Precision: 0.7936, Recall: 0.9933, F1-score: 0.8823\n",
      "TP: 296, FP: 77, TN: 425, FN: 2\n",
      "Epoch 74, Loss: 43.48789978027344\n",
      "Test Loss: 58.1435, Test Accuracy: 0.9050\n",
      "Precision: 0.8000, Recall: 0.9933, F1-score: 0.8862\n",
      "TP: 296, FP: 74, TN: 428, FN: 2\n",
      "Epoch 75, Loss: 43.04344727198283\n",
      "Test Loss: 58.8637, Test Accuracy: 0.9075\n",
      "Precision: 0.8043, Recall: 0.9933, F1-score: 0.8889\n",
      "TP: 296, FP: 72, TN: 430, FN: 2\n",
      "Epoch 76, Loss: 42.71629814783732\n",
      "Test Loss: 57.4214, Test Accuracy: 0.9038\n",
      "Precision: 0.7978, Recall: 0.9933, F1-score: 0.8849\n",
      "TP: 296, FP: 75, TN: 427, FN: 2\n",
      "Epoch 77, Loss: 42.28247042338053\n",
      "Test Loss: 57.4420, Test Accuracy: 0.9012\n",
      "Precision: 0.7936, Recall: 0.9933, F1-score: 0.8823\n",
      "TP: 296, FP: 77, TN: 425, FN: 2\n",
      "Epoch 78, Loss: 42.21009771347046\n",
      "Test Loss: 58.1290, Test Accuracy: 0.9075\n",
      "Precision: 0.8043, Recall: 0.9933, F1-score: 0.8889\n",
      "TP: 296, FP: 72, TN: 430, FN: 2\n",
      "Epoch 79, Loss: 41.87695957819621\n",
      "Test Loss: 56.4992, Test Accuracy: 0.9038\n",
      "Precision: 0.7978, Recall: 0.9933, F1-score: 0.8849\n",
      "TP: 296, FP: 75, TN: 427, FN: 2\n",
      "Epoch 80, Loss: 41.27389270146688\n",
      "Test Loss: 57.3041, Test Accuracy: 0.9100\n",
      "Precision: 0.8087, Recall: 0.9933, F1-score: 0.8916\n",
      "TP: 296, FP: 70, TN: 432, FN: 2\n",
      "Epoch 81, Loss: 40.70004254659017\n",
      "Test Loss: 56.3317, Test Accuracy: 0.9075\n",
      "Precision: 0.8043, Recall: 0.9933, F1-score: 0.8889\n",
      "TP: 296, FP: 72, TN: 430, FN: 2\n",
      "Epoch 82, Loss: 40.80516425450643\n",
      "Test Loss: 56.6320, Test Accuracy: 0.9137\n",
      "Precision: 0.8154, Recall: 0.9933, F1-score: 0.8956\n",
      "TP: 296, FP: 67, TN: 435, FN: 2\n",
      "Epoch 83, Loss: 40.74209718704223\n",
      "Test Loss: 55.9491, Test Accuracy: 0.9150\n",
      "Precision: 0.8177, Recall: 0.9933, F1-score: 0.8970\n",
      "TP: 296, FP: 66, TN: 436, FN: 2\n",
      "Epoch 84, Loss: 40.49035954157512\n",
      "Test Loss: 54.9709, Test Accuracy: 0.9100\n",
      "Precision: 0.8087, Recall: 0.9933, F1-score: 0.8916\n",
      "TP: 296, FP: 70, TN: 432, FN: 2\n",
      "Epoch 85, Loss: 40.10465902964274\n",
      "Test Loss: 55.2803, Test Accuracy: 0.9150\n",
      "Precision: 0.8177, Recall: 0.9933, F1-score: 0.8970\n",
      "TP: 296, FP: 66, TN: 436, FN: 2\n",
      "Epoch 86, Loss: 39.52108074188232\n",
      "Test Loss: 54.1829, Test Accuracy: 0.9150\n",
      "Precision: 0.8177, Recall: 0.9933, F1-score: 0.8970\n",
      "TP: 296, FP: 66, TN: 436, FN: 2\n",
      "Epoch 87, Loss: 39.31983834584554\n",
      "Test Loss: 53.9993, Test Accuracy: 0.9187\n",
      "Precision: 0.8245, Recall: 0.9933, F1-score: 0.9011\n",
      "TP: 296, FP: 63, TN: 439, FN: 2\n",
      "Epoch 88, Loss: 39.40655255635579\n",
      "Test Loss: 55.2170, Test Accuracy: 0.9175\n",
      "Precision: 0.8222, Recall: 0.9933, F1-score: 0.8997\n",
      "TP: 296, FP: 64, TN: 438, FN: 2\n",
      "Epoch 89, Loss: 38.88561574935913\n",
      "Test Loss: 53.3521, Test Accuracy: 0.9125\n",
      "Precision: 0.8132, Recall: 0.9933, F1-score: 0.8943\n",
      "TP: 296, FP: 68, TN: 434, FN: 2\n",
      "Epoch 90, Loss: 38.513618303934734\n",
      "Test Loss: 53.7741, Test Accuracy: 0.9175\n",
      "Precision: 0.8222, Recall: 0.9933, F1-score: 0.8997\n",
      "TP: 296, FP: 64, TN: 438, FN: 2\n",
      "Epoch 91, Loss: 38.342050863901775\n",
      "Test Loss: 52.4786, Test Accuracy: 0.9187\n",
      "Precision: 0.8245, Recall: 0.9933, F1-score: 0.9011\n",
      "TP: 296, FP: 63, TN: 439, FN: 2\n",
      "Epoch 92, Loss: 37.82766455332438\n",
      "Test Loss: 52.5801, Test Accuracy: 0.9250\n",
      "Precision: 0.8362, Recall: 0.9933, F1-score: 0.9080\n",
      "TP: 296, FP: 58, TN: 444, FN: 2\n",
      "Epoch 93, Loss: 38.156325848897296\n",
      "Test Loss: 53.4853, Test Accuracy: 0.9225\n",
      "Precision: 0.8315, Recall: 0.9933, F1-score: 0.9052\n",
      "TP: 296, FP: 60, TN: 442, FN: 2\n",
      "Epoch 94, Loss: 37.76671148300171\n",
      "Test Loss: 52.5528, Test Accuracy: 0.9250\n",
      "Precision: 0.8362, Recall: 0.9933, F1-score: 0.9080\n",
      "TP: 296, FP: 58, TN: 444, FN: 2\n",
      "Epoch 95, Loss: 37.385486927032474\n",
      "Test Loss: 51.2710, Test Accuracy: 0.9225\n",
      "Precision: 0.8315, Recall: 0.9933, F1-score: 0.9052\n",
      "TP: 296, FP: 60, TN: 442, FN: 2\n",
      "Epoch 96, Loss: 37.1100137201945\n",
      "Test Loss: 50.7826, Test Accuracy: 0.9175\n",
      "Precision: 0.8222, Recall: 0.9933, F1-score: 0.8997\n",
      "TP: 296, FP: 64, TN: 438, FN: 2\n",
      "Epoch 97, Loss: 37.09261076609294\n",
      "Test Loss: 50.8034, Test Accuracy: 0.9275\n",
      "Precision: 0.8409, Recall: 0.9933, F1-score: 0.9108\n",
      "TP: 296, FP: 56, TN: 446, FN: 2\n",
      "Epoch 98, Loss: 36.5981657854716\n",
      "Test Loss: 50.9641, Test Accuracy: 0.9250\n",
      "Precision: 0.8362, Recall: 0.9933, F1-score: 0.9080\n",
      "TP: 296, FP: 58, TN: 444, FN: 2\n",
      "Epoch 99, Loss: 36.690274976094564\n",
      "Test Loss: 50.3321, Test Accuracy: 0.9275\n",
      "Precision: 0.8409, Recall: 0.9933, F1-score: 0.9108\n",
      "TP: 296, FP: 56, TN: 446, FN: 2\n",
      "Epoch 100, Loss: 36.19977284749349\n",
      "Test Loss: 49.9684, Test Accuracy: 0.9250\n",
      "Precision: 0.8362, Recall: 0.9933, F1-score: 0.9080\n",
      "TP: 296, FP: 58, TN: 444, FN: 2\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "stats = dict()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = trainLoop(autoencoder, optimizer, criterion, train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "    pred_function = lambda x: x[2] < MARGIN\n",
    "    testLoop(autoencoder, criterion, test_loader, pred_function, stats)\n",
    "\n",
    "save_stats(\"AutoEncoder_V1_bert_base\", stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOnlyMLP(nn.Module):\n",
    "    def __init__(self, encoder, mlp):\n",
    "        super(EncoderOnlyMLP, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.mlp = mlp\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.encoder(x1)\n",
    "        x2 = self.encoder(x2)\n",
    "        diff = torch.abs(x1 - x2)\n",
    "        x = self.mlp(diff)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, label):\n",
    "        loss_1 = label * torch.log(outputs)\n",
    "        loss_2 = (1-label) * torch.log(1 - outputs)\n",
    "        return torch.mean(-(loss_1 + loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(48, 256, 1)\n",
    "encoder_only_mlp = EncoderOnlyMLP(autoencoder.encoder, mlp)\n",
    "criterion = ClassificationLoss()\n",
    "optimizer = optim.SGD(encoder_only_mlp.parameters(), lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.35346180215477946\n",
      "Test Loss: 0.3686, Test Accuracy: 0.6275\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "TP: 0, FP: 0, TN: 502, FN: 298\n",
      "Epoch 2, Loss: 0.29436530464639266\n",
      "Test Loss: 0.3657, Test Accuracy: 0.6275\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "TP: 0, FP: 0, TN: 502, FN: 298\n",
      "Epoch 3, Loss: 0.2873201160815855\n",
      "Test Loss: 0.3616, Test Accuracy: 0.6275\n",
      "Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "TP: 0, FP: 0, TN: 502, FN: 298\n",
      "Epoch 4, Loss: 0.28163700498019656\n",
      "Test Loss: 0.3566, Test Accuracy: 0.6362\n",
      "Precision: 1.0000, Recall: 0.0235, F1-score: 0.0459\n",
      "TP: 7, FP: 0, TN: 502, FN: 291\n",
      "Epoch 5, Loss: 0.2760693063400686\n",
      "Test Loss: 0.3516, Test Accuracy: 0.6450\n",
      "Precision: 1.0000, Recall: 0.0470, F1-score: 0.0897\n",
      "TP: 14, FP: 0, TN: 502, FN: 284\n",
      "Epoch 6, Loss: 0.27077720352758966\n",
      "Test Loss: 0.3464, Test Accuracy: 0.6538\n",
      "Precision: 1.0000, Recall: 0.0705, F1-score: 0.1317\n",
      "TP: 21, FP: 0, TN: 502, FN: 277\n",
      "Epoch 7, Loss: 0.26564648034982385\n",
      "Test Loss: 0.3411, Test Accuracy: 0.6650\n",
      "Precision: 1.0000, Recall: 0.1007, F1-score: 0.1829\n",
      "TP: 30, FP: 0, TN: 502, FN: 268\n",
      "Epoch 8, Loss: 0.26084799648883444\n",
      "Test Loss: 0.3359, Test Accuracy: 0.6850\n",
      "Precision: 1.0000, Recall: 0.1544, F1-score: 0.2674\n",
      "TP: 46, FP: 0, TN: 502, FN: 252\n",
      "Epoch 9, Loss: 0.25587257036628824\n",
      "Test Loss: 0.3309, Test Accuracy: 0.7137\n",
      "Precision: 1.0000, Recall: 0.2315, F1-score: 0.3760\n",
      "TP: 69, FP: 0, TN: 502, FN: 229\n",
      "Epoch 10, Loss: 0.25104052492417395\n",
      "Test Loss: 0.3262, Test Accuracy: 0.7400\n",
      "Precision: 1.0000, Recall: 0.3020, F1-score: 0.4639\n",
      "TP: 90, FP: 0, TN: 502, FN: 208\n",
      "Epoch 11, Loss: 0.24660698465071618\n",
      "Test Loss: 0.3213, Test Accuracy: 0.7937\n",
      "Precision: 1.0000, Recall: 0.4463, F1-score: 0.6172\n",
      "TP: 133, FP: 0, TN: 502, FN: 165\n",
      "Epoch 12, Loss: 0.24191050739803663\n",
      "Test Loss: 0.3167, Test Accuracy: 0.8200\n",
      "Precision: 1.0000, Recall: 0.5168, F1-score: 0.6814\n",
      "TP: 154, FP: 0, TN: 502, FN: 144\n",
      "Epoch 13, Loss: 0.2376254542171955\n",
      "Test Loss: 0.3122, Test Accuracy: 0.8450\n",
      "Precision: 1.0000, Recall: 0.5839, F1-score: 0.7373\n",
      "TP: 174, FP: 0, TN: 502, FN: 124\n",
      "Epoch 14, Loss: 0.23319573736439148\n",
      "Test Loss: 0.3078, Test Accuracy: 0.8562\n",
      "Precision: 1.0000, Recall: 0.6141, F1-score: 0.7609\n",
      "TP: 183, FP: 0, TN: 502, FN: 115\n",
      "Epoch 15, Loss: 0.22928109591826795\n",
      "Test Loss: 0.3036, Test Accuracy: 0.8650\n",
      "Precision: 1.0000, Recall: 0.6376, F1-score: 0.7787\n",
      "TP: 190, FP: 0, TN: 502, FN: 108\n",
      "Epoch 16, Loss: 0.22495827225813023\n",
      "Test Loss: 0.2993, Test Accuracy: 0.8762\n",
      "Precision: 1.0000, Recall: 0.6678, F1-score: 0.8008\n",
      "TP: 199, FP: 0, TN: 502, FN: 99\n",
      "Epoch 17, Loss: 0.2210185859693835\n",
      "Test Loss: 0.2953, Test Accuracy: 0.8900\n",
      "Precision: 1.0000, Recall: 0.7047, F1-score: 0.8268\n",
      "TP: 210, FP: 0, TN: 502, FN: 88\n",
      "Epoch 18, Loss: 0.21696358146611602\n",
      "Test Loss: 0.2913, Test Accuracy: 0.8938\n",
      "Precision: 1.0000, Recall: 0.7148, F1-score: 0.8337\n",
      "TP: 213, FP: 0, TN: 502, FN: 85\n",
      "Epoch 19, Loss: 0.21318309947072217\n",
      "Test Loss: 0.2875, Test Accuracy: 0.9038\n",
      "Precision: 1.0000, Recall: 0.7416, F1-score: 0.8516\n",
      "TP: 221, FP: 0, TN: 502, FN: 77\n",
      "Epoch 20, Loss: 0.20949346483568662\n",
      "Test Loss: 0.2838, Test Accuracy: 0.9113\n",
      "Precision: 1.0000, Recall: 0.7617, F1-score: 0.8648\n",
      "TP: 227, FP: 0, TN: 502, FN: 71\n",
      "Epoch 21, Loss: 0.2056448786209027\n",
      "Test Loss: 0.2801, Test Accuracy: 0.9250\n",
      "Precision: 1.0000, Recall: 0.7987, F1-score: 0.8881\n",
      "TP: 238, FP: 0, TN: 502, FN: 60\n",
      "Epoch 22, Loss: 0.2020278901948283\n",
      "Test Loss: 0.2764, Test Accuracy: 0.9350\n",
      "Precision: 1.0000, Recall: 0.8255, F1-score: 0.9044\n",
      "TP: 246, FP: 0, TN: 502, FN: 52\n",
      "Epoch 23, Loss: 0.1986408058204688\n",
      "Test Loss: 0.2728, Test Accuracy: 0.9400\n",
      "Precision: 1.0000, Recall: 0.8389, F1-score: 0.9124\n",
      "TP: 250, FP: 0, TN: 502, FN: 48\n",
      "Epoch 24, Loss: 0.19503594546268382\n",
      "Test Loss: 0.2694, Test Accuracy: 0.9425\n",
      "Precision: 1.0000, Recall: 0.8456, F1-score: 0.9164\n",
      "TP: 252, FP: 0, TN: 502, FN: 46\n",
      "Epoch 25, Loss: 0.19165144177153706\n",
      "Test Loss: 0.2661, Test Accuracy: 0.9450\n",
      "Precision: 1.0000, Recall: 0.8523, F1-score: 0.9203\n",
      "TP: 254, FP: 0, TN: 502, FN: 44\n",
      "Epoch 26, Loss: 0.18841712119756265\n",
      "Test Loss: 0.2627, Test Accuracy: 0.9513\n",
      "Precision: 1.0000, Recall: 0.8691, F1-score: 0.9300\n",
      "TP: 259, FP: 0, TN: 502, FN: 39\n",
      "Epoch 27, Loss: 0.1851744966271023\n",
      "Test Loss: 0.2595, Test Accuracy: 0.9550\n",
      "Precision: 1.0000, Recall: 0.8792, F1-score: 0.9357\n",
      "TP: 262, FP: 0, TN: 502, FN: 36\n",
      "Epoch 28, Loss: 0.18211040725970332\n",
      "Test Loss: 0.2563, Test Accuracy: 0.9587\n",
      "Precision: 1.0000, Recall: 0.8893, F1-score: 0.9414\n",
      "TP: 265, FP: 0, TN: 502, FN: 33\n",
      "Epoch 29, Loss: 0.17889392568807427\n",
      "Test Loss: 0.2533, Test Accuracy: 0.9613\n",
      "Precision: 1.0000, Recall: 0.8960, F1-score: 0.9451\n",
      "TP: 267, FP: 0, TN: 502, FN: 31\n",
      "Epoch 30, Loss: 0.1756864564338078\n",
      "Test Loss: 0.2503, Test Accuracy: 0.9650\n",
      "Precision: 1.0000, Recall: 0.9060, F1-score: 0.9507\n",
      "TP: 270, FP: 0, TN: 502, FN: 28\n",
      "Epoch 31, Loss: 0.1728070926425668\n",
      "Test Loss: 0.2474, Test Accuracy: 0.9650\n",
      "Precision: 1.0000, Recall: 0.9060, F1-score: 0.9507\n",
      "TP: 270, FP: 0, TN: 502, FN: 28\n",
      "Epoch 32, Loss: 0.1698074670524026\n",
      "Test Loss: 0.2446, Test Accuracy: 0.9663\n",
      "Precision: 1.0000, Recall: 0.9094, F1-score: 0.9525\n",
      "TP: 271, FP: 0, TN: 502, FN: 27\n",
      "Epoch 33, Loss: 0.16705218670579294\n",
      "Test Loss: 0.2417, Test Accuracy: 0.9688\n",
      "Precision: 1.0000, Recall: 0.9161, F1-score: 0.9562\n",
      "TP: 273, FP: 0, TN: 502, FN: 25\n",
      "Epoch 34, Loss: 0.1641911838731418\n",
      "Test Loss: 0.2388, Test Accuracy: 0.9688\n",
      "Precision: 1.0000, Recall: 0.9161, F1-score: 0.9562\n",
      "TP: 273, FP: 0, TN: 502, FN: 25\n",
      "Epoch 35, Loss: 0.16139212073913464\n",
      "Test Loss: 0.2363, Test Accuracy: 0.9688\n",
      "Precision: 1.0000, Recall: 0.9161, F1-score: 0.9562\n",
      "TP: 273, FP: 0, TN: 502, FN: 25\n",
      "Epoch 36, Loss: 0.15868961846611152\n",
      "Test Loss: 0.2336, Test Accuracy: 0.9688\n",
      "Precision: 1.0000, Recall: 0.9161, F1-score: 0.9562\n",
      "TP: 273, FP: 0, TN: 502, FN: 25\n",
      "Epoch 37, Loss: 0.1560373607017876\n",
      "Test Loss: 0.2309, Test Accuracy: 0.9700\n",
      "Precision: 1.0000, Recall: 0.9195, F1-score: 0.9580\n",
      "TP: 274, FP: 0, TN: 502, FN: 24\n",
      "Epoch 38, Loss: 0.1534491635983189\n",
      "Test Loss: 0.2284, Test Accuracy: 0.9712\n",
      "Precision: 1.0000, Recall: 0.9228, F1-score: 0.9599\n",
      "TP: 275, FP: 0, TN: 502, FN: 23\n",
      "Epoch 39, Loss: 0.15085571569167466\n",
      "Test Loss: 0.2259, Test Accuracy: 0.9712\n",
      "Precision: 1.0000, Recall: 0.9228, F1-score: 0.9599\n",
      "TP: 275, FP: 0, TN: 502, FN: 23\n",
      "Epoch 40, Loss: 0.14843520863990609\n",
      "Test Loss: 0.2233, Test Accuracy: 0.9725\n",
      "Precision: 1.0000, Recall: 0.9262, F1-score: 0.9617\n",
      "TP: 276, FP: 0, TN: 502, FN: 22\n",
      "Epoch 41, Loss: 0.14590218724838147\n",
      "Test Loss: 0.2209, Test Accuracy: 0.9712\n",
      "Precision: 0.9964, Recall: 0.9262, F1-score: 0.9600\n",
      "TP: 276, FP: 1, TN: 501, FN: 22\n",
      "Epoch 42, Loss: 0.14360993260440105\n",
      "Test Loss: 0.2187, Test Accuracy: 0.9712\n",
      "Precision: 0.9964, Recall: 0.9262, F1-score: 0.9600\n",
      "TP: 276, FP: 1, TN: 501, FN: 22\n",
      "Epoch 43, Loss: 0.14119945970422124\n",
      "Test Loss: 0.2164, Test Accuracy: 0.9725\n",
      "Precision: 0.9964, Recall: 0.9295, F1-score: 0.9618\n",
      "TP: 277, FP: 1, TN: 501, FN: 21\n",
      "Epoch 44, Loss: 0.13894418202224187\n",
      "Test Loss: 0.2142, Test Accuracy: 0.9725\n",
      "Precision: 0.9964, Recall: 0.9295, F1-score: 0.9618\n",
      "TP: 277, FP: 1, TN: 501, FN: 21\n",
      "Epoch 45, Loss: 0.1366948452712192\n",
      "Test Loss: 0.2120, Test Accuracy: 0.9725\n",
      "Precision: 0.9964, Recall: 0.9295, F1-score: 0.9618\n",
      "TP: 277, FP: 1, TN: 501, FN: 21\n",
      "Epoch 46, Loss: 0.13433966111391782\n",
      "Test Loss: 0.2098, Test Accuracy: 0.9750\n",
      "Precision: 0.9964, Recall: 0.9362, F1-score: 0.9654\n",
      "TP: 279, FP: 1, TN: 501, FN: 19\n",
      "Epoch 47, Loss: 0.13230278211956223\n",
      "Test Loss: 0.2078, Test Accuracy: 0.9750\n",
      "Precision: 0.9964, Recall: 0.9362, F1-score: 0.9654\n",
      "TP: 279, FP: 1, TN: 501, FN: 19\n",
      "Epoch 48, Loss: 0.1302033477082538\n",
      "Test Loss: 0.2058, Test Accuracy: 0.9750\n",
      "Precision: 0.9964, Recall: 0.9362, F1-score: 0.9654\n",
      "TP: 279, FP: 1, TN: 501, FN: 19\n",
      "Epoch 49, Loss: 0.12814744119221966\n",
      "Test Loss: 0.2038, Test Accuracy: 0.9750\n",
      "Precision: 0.9964, Recall: 0.9362, F1-score: 0.9654\n",
      "TP: 279, FP: 1, TN: 501, FN: 19\n",
      "Epoch 50, Loss: 0.1259490995380717\n",
      "Test Loss: 0.2020, Test Accuracy: 0.9750\n",
      "Precision: 0.9964, Recall: 0.9362, F1-score: 0.9654\n",
      "TP: 279, FP: 1, TN: 501, FN: 19\n",
      "Epoch 51, Loss: 0.12408717858294646\n",
      "Test Loss: 0.2001, Test Accuracy: 0.9750\n",
      "Precision: 0.9964, Recall: 0.9362, F1-score: 0.9654\n",
      "TP: 279, FP: 1, TN: 501, FN: 19\n",
      "Epoch 52, Loss: 0.12210117627245684\n",
      "Test Loss: 0.1982, Test Accuracy: 0.9738\n",
      "Precision: 0.9929, Recall: 0.9362, F1-score: 0.9637\n",
      "TP: 279, FP: 2, TN: 500, FN: 19\n",
      "Epoch 53, Loss: 0.1202601380397876\n",
      "Test Loss: 0.1963, Test Accuracy: 0.9738\n",
      "Precision: 0.9929, Recall: 0.9362, F1-score: 0.9637\n",
      "TP: 279, FP: 2, TN: 500, FN: 19\n",
      "Epoch 54, Loss: 0.11838422799444137\n",
      "Test Loss: 0.1944, Test Accuracy: 0.9738\n",
      "Precision: 0.9929, Recall: 0.9362, F1-score: 0.9637\n",
      "TP: 279, FP: 2, TN: 500, FN: 19\n",
      "Epoch 55, Loss: 0.11652291118050925\n",
      "Test Loss: 0.1929, Test Accuracy: 0.9750\n",
      "Precision: 0.9929, Recall: 0.9396, F1-score: 0.9655\n",
      "TP: 280, FP: 2, TN: 500, FN: 18\n",
      "Epoch 56, Loss: 0.11467070448367546\n",
      "Test Loss: 0.1912, Test Accuracy: 0.9750\n",
      "Precision: 0.9929, Recall: 0.9396, F1-score: 0.9655\n",
      "TP: 280, FP: 2, TN: 500, FN: 18\n",
      "Epoch 57, Loss: 0.11290761289730047\n",
      "Test Loss: 0.1895, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 58, Loss: 0.11127888765263681\n",
      "Test Loss: 0.1879, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 59, Loss: 0.10958509406618153\n",
      "Test Loss: 0.1864, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 60, Loss: 0.1078932963469803\n",
      "Test Loss: 0.1848, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 61, Loss: 0.10625525799929164\n",
      "Test Loss: 0.1832, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 62, Loss: 0.10469253740894298\n",
      "Test Loss: 0.1819, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 63, Loss: 0.10306601126018601\n",
      "Test Loss: 0.1804, Test Accuracy: 0.9762\n",
      "Precision: 0.9929, Recall: 0.9430, F1-score: 0.9673\n",
      "TP: 281, FP: 2, TN: 500, FN: 17\n",
      "Epoch 64, Loss: 0.10168849836355852\n",
      "Test Loss: 0.1790, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 65, Loss: 0.10015408986752543\n",
      "Test Loss: 0.1776, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 66, Loss: 0.09870257173043986\n",
      "Test Loss: 0.1763, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 67, Loss: 0.09726498257834465\n",
      "Test Loss: 0.1750, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 68, Loss: 0.0958552905212855\n",
      "Test Loss: 0.1737, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 69, Loss: 0.09447918138156335\n",
      "Test Loss: 0.1724, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 70, Loss: 0.09324319983211657\n",
      "Test Loss: 0.1711, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 71, Loss: 0.09184476672206074\n",
      "Test Loss: 0.1699, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 72, Loss: 0.09056624702721214\n",
      "Test Loss: 0.1688, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 73, Loss: 0.08936179668332139\n",
      "Test Loss: 0.1678, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 74, Loss: 0.08811393408779016\n",
      "Test Loss: 0.1666, Test Accuracy: 0.9775\n",
      "Precision: 0.9930, Recall: 0.9463, F1-score: 0.9691\n",
      "TP: 282, FP: 2, TN: 500, FN: 16\n",
      "Epoch 75, Loss: 0.08686410855967551\n",
      "Test Loss: 0.1655, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 76, Loss: 0.0856954893283546\n",
      "Test Loss: 0.1644, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 77, Loss: 0.08458804149568702\n",
      "Test Loss: 0.1633, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 78, Loss: 0.08336904947257912\n",
      "Test Loss: 0.1624, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 79, Loss: 0.082223196158108\n",
      "Test Loss: 0.1614, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 80, Loss: 0.08118157017612247\n",
      "Test Loss: 0.1604, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 81, Loss: 0.08014311444654595\n",
      "Test Loss: 0.1594, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 82, Loss: 0.07900002689100802\n",
      "Test Loss: 0.1585, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 83, Loss: 0.07803356200146179\n",
      "Test Loss: 0.1577, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 84, Loss: 0.07706784803265085\n",
      "Test Loss: 0.1568, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 85, Loss: 0.07602430359169375\n",
      "Test Loss: 0.1559, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 86, Loss: 0.07511942587792873\n",
      "Test Loss: 0.1550, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 87, Loss: 0.07417124142171815\n",
      "Test Loss: 0.1542, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 88, Loss: 0.07319622605340555\n",
      "Test Loss: 0.1534, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 89, Loss: 0.07226797318338261\n",
      "Test Loss: 0.1527, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 90, Loss: 0.07137728050972024\n",
      "Test Loss: 0.1519, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 91, Loss: 0.07054579764963516\n",
      "Test Loss: 0.1511, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 92, Loss: 0.06963925595531086\n",
      "Test Loss: 0.1504, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 93, Loss: 0.06888354167419797\n",
      "Test Loss: 0.1497, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 94, Loss: 0.06797690028790385\n",
      "Test Loss: 0.1490, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 95, Loss: 0.06716349293710663\n",
      "Test Loss: 0.1483, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 96, Loss: 0.0664493537026768\n",
      "Test Loss: 0.1477, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 97, Loss: 0.06556851097091566\n",
      "Test Loss: 0.1471, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 98, Loss: 0.06484929994485962\n",
      "Test Loss: 0.1464, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 99, Loss: 0.06407338530678923\n",
      "Test Loss: 0.1458, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n",
      "Epoch 100, Loss: 0.06334045308913726\n",
      "Test Loss: 0.1452, Test Accuracy: 0.9762\n",
      "Precision: 0.9895, Recall: 0.9463, F1-score: 0.9674\n",
      "TP: 282, FP: 3, TN: 499, FN: 16\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "stats = dict()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = trainLoop(encoder_only_mlp, optimizer, criterion, train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss}\")\n",
    "    pred_function = lambda x: x > 0.5\n",
    "    testLoop(encoder_only_mlp, criterion, test_loader, pred_function, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_stats(\"AutoEncoder_V2_bert_base\", stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
