{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/Atcs_rl/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(path: str):\n",
    "    with open(path, 'r', encoding='utf-8') as json_file:\n",
    "        try:\n",
    "            return json.load(json_file)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error reading {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(json_as_dict: dict):\n",
    "    text = json_as_dict[\"<page title>\"]\n",
    "        \n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states[0]\n",
    "\n",
    "    cls_embedding = token_embeddings[0].detach().numpy()\n",
    "\n",
    "    return cls_embedding\n",
    "\n",
    "def get_embedding(json_as_dict: dict):\n",
    "    text = \"\"\n",
    "    for key in json_as_dict:\n",
    "        value = json_as_dict[key]\n",
    "        if isinstance(value, list):\n",
    "            for elem in value:\n",
    "                text += (\" \" + elem.lower())\n",
    "        else:\n",
    "            text += (\" \" + value.lower())\n",
    "        \n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    token_embeddings = last_hidden_states[0]\n",
    "\n",
    "    cls_embedding = token_embeddings[0]\n",
    "    # mean_embedding = torch.mean(token_embeddings, dim=0)\n",
    "\n",
    "    return cls_embedding.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111156"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "path2id = dict()\n",
    "id2embedding = dict()\n",
    "id2title = dict()\n",
    "\n",
    "DATASET_FOLDER_PATH = 'dataset/'\n",
    "GROUND_TRUTH_PATH = DATASET_FOLDER_PATH + 'monitor_entity_resolution_labelled.csv'\n",
    "SOURCES_PATH = DATASET_FOLDER_PATH + \"2013_monitor_specs/\"\n",
    "\n",
    "numberOfZeros = 0\n",
    "numberOfOnes = 0\n",
    "\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "with open(GROUND_TRUTH_PATH, mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for index, row in enumerate(csv_reader):\n",
    "        print(\"\\r\"+str(index), end=\"\")\n",
    "        if index == 0:\n",
    "            columns = row\n",
    "            continue\n",
    "        left_folder, left_id = row[0].split(\"//\")\n",
    "        right_folder, right_id = row[1].split(\"//\")\n",
    "        label = int(row[2])\n",
    "\n",
    "        if label == 0: numberOfZeros += 1\n",
    "        else: numberOfOnes += 1\n",
    "\n",
    "        left_path = SOURCES_PATH + left_folder + \"/\" + left_id + \".json\"\n",
    "        right_path = SOURCES_PATH + right_folder + \"/\" + right_id + \".json\"\n",
    "\n",
    "        if left_path in path2id:\n",
    "            left_id = path2id[left_path]\n",
    "        else:\n",
    "            left_id = len(path2id)\n",
    "            left_json = load_json(left_path)\n",
    "            id2embedding[left_id] = get_embedding(left_json)\n",
    "            id2title[left_id] = left_json[\"<page title>\"]\n",
    "            path2id[left_path] = left_id\n",
    "\n",
    "        if right_path in path2id:\n",
    "            right_id = path2id[right_path]\n",
    "        else:\n",
    "            right_id = len(path2id)\n",
    "            right_json = load_json(right_path)\n",
    "            id2embedding[right_id] = get_embedding(right_json)\n",
    "            id2title[right_id] = left_json[\"<page title>\"]\n",
    "            path2id[right_path] = right_id\n",
    "        \n",
    "        if label == 1:\n",
    "            positives.append([left_id, right_id, label])\n",
    "        else:\n",
    "            negatives.append([left_id, right_id, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073\n",
      "2125\n",
      "3198\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "total_number = len(positives) + len(negatives)\n",
    "pos = pd.DataFrame(positives, columns=columns)\n",
    "neg = pd.DataFrame(negatives, columns=columns)\n",
    "neg = neg.sample(frac=2*(len(positives)/total_number)).reset_index(drop=True)\n",
    "\n",
    "print(len(pos))\n",
    "print(len(neg))\n",
    "\n",
    "result = pd.concat([pos, neg], axis=0, ignore_index=True)\n",
    "result = result.sample(frac=1).reset_index(drop=True)\n",
    "print(len(result))\n",
    "\n",
    "SAVES_FOLDER = \"saves/\"\n",
    "result.to_csv(SAVES_FOLDER + 'dataset.csv', index=False)\n",
    "\n",
    "with open(SAVES_FOLDER + 'id2embedding.pkl', 'wb') as f:\n",
    "    pickle.dump(id2embedding, f)\n",
    "\n",
    "with open(SAVES_FOLDER + 'id2title.pkl', 'wb') as f:\n",
    "    pickle.dump(id2title, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(id2embedding[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Atcs_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
